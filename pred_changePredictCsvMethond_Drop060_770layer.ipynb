{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"all.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"train.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref = zipfile.ZipFile(\"test.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GlobalAveragePooling2D, Dropout, Lambda\n",
    "from keras.applications import inception_resnet_v2\n",
    "# from keras.preprocessing.image import *\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:57<00:00, 218.61it/s]\n"
     ]
    }
   ],
   "source": [
    "n = 25000\n",
    "img_height, img_width = 299, 299\n",
    "X = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, 1), dtype=np.uint8)\n",
    "\n",
    "for i in tqdm(range(12500)): # n/2\n",
    "    X[i] = cv2.resize(cv2.imread('train/cat.%d.jpg' % i), (img_height, img_width))\n",
    "    X[i+12500] = cv2.resize(cv2.imread('train/dog.%d.jpg' % i), (img_height, img_width)) # n/2\n",
    "\n",
    "# 打上标签\n",
    "y[12500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_input_tensor = Input((img_height, img_width, 3))\n",
    "# x_input_tensor = inception_resnet_v2.preprocess_input(i_input_tensor)\t# 错误的处理\n",
    "x_input_tensor = Lambda(inception_resnet_v2.preprocess_input)(i_input_tensor) # 建立模型预处理\n",
    "\n",
    "# 搭建基础模型\n",
    "base_model = inception_resnet_v2.InceptionResNetV2(input_tensor=x_input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "范例：不运行\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "如果验证损失下降， 那么在每个训练轮之后保存模型。\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_checkpint_060_770.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 锁定所有神经网络层\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "i_output = GlobalAveragePooling2D()(base_model.output)\n",
    "i_output = Dropout(0.6)(i_output) # 0.25 比 0.5 好\n",
    "i_predictions = Dense(1, activation='sigmoid')(i_output)\n",
    "model = Model(base_model.input, i_predictions)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 168s 8ms/step - loss: 0.1696 - acc: 0.9370 - val_loss: 0.0768 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07681, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1141 - acc: 0.9549 - val_loss: 0.0694 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07681 to 0.06941, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1101 - acc: 0.9593 - val_loss: 0.0884 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06941\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1076 - acc: 0.9601 - val_loss: 0.0966 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06941\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1020 - acc: 0.9612 - val_loss: 0.0766 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06941\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1032 - acc: 0.9609 - val_loss: 0.0694 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06941 to 0.06936, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0994 - acc: 0.9621 - val_loss: 0.0567 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06936 to 0.05668, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 148s 7ms/step - loss: 0.1084 - acc: 0.9583 - val_loss: 0.0664 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6e7d9e400>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20,     # defalt 16\n",
    "\tepochs=8, \t# defalt 5\n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5 0\n",
      "lambda_5 1\n",
      "conv2d_813 2\n",
      "batch_normalization_813 3\n",
      "activation_813 4\n",
      "conv2d_814 5\n",
      "batch_normalization_814 6\n",
      "activation_814 7\n",
      "conv2d_815 8\n",
      "batch_normalization_815 9\n",
      "activation_815 10\n",
      "max_pooling2d_17 11\n",
      "conv2d_816 12\n",
      "batch_normalization_816 13\n",
      "activation_816 14\n",
      "conv2d_817 15\n",
      "batch_normalization_817 16\n",
      "activation_817 17\n",
      "max_pooling2d_18 18\n",
      "conv2d_821 19\n",
      "batch_normalization_821 20\n",
      "activation_821 21\n",
      "conv2d_819 22\n",
      "conv2d_822 23\n",
      "batch_normalization_819 24\n",
      "batch_normalization_822 25\n",
      "activation_819 26\n",
      "activation_822 27\n",
      "average_pooling2d_5 28\n",
      "conv2d_818 29\n",
      "conv2d_820 30\n",
      "conv2d_823 31\n",
      "conv2d_824 32\n",
      "batch_normalization_818 33\n",
      "batch_normalization_820 34\n",
      "batch_normalization_823 35\n",
      "batch_normalization_824 36\n",
      "activation_818 37\n",
      "activation_820 38\n",
      "activation_823 39\n",
      "activation_824 40\n",
      "mixed_5b 41\n",
      "conv2d_828 42\n",
      "batch_normalization_828 43\n",
      "activation_828 44\n",
      "conv2d_826 45\n",
      "conv2d_829 46\n",
      "batch_normalization_826 47\n",
      "batch_normalization_829 48\n",
      "activation_826 49\n",
      "activation_829 50\n",
      "conv2d_825 51\n",
      "conv2d_827 52\n",
      "conv2d_830 53\n",
      "batch_normalization_825 54\n",
      "batch_normalization_827 55\n",
      "batch_normalization_830 56\n",
      "activation_825 57\n",
      "activation_827 58\n",
      "activation_830 59\n",
      "block35_1_mixed 60\n",
      "block35_1_conv 61\n",
      "block35_1 62\n",
      "block35_1_ac 63\n",
      "conv2d_834 64\n",
      "batch_normalization_834 65\n",
      "activation_834 66\n",
      "conv2d_832 67\n",
      "conv2d_835 68\n",
      "batch_normalization_832 69\n",
      "batch_normalization_835 70\n",
      "activation_832 71\n",
      "activation_835 72\n",
      "conv2d_831 73\n",
      "conv2d_833 74\n",
      "conv2d_836 75\n",
      "batch_normalization_831 76\n",
      "batch_normalization_833 77\n",
      "batch_normalization_836 78\n",
      "activation_831 79\n",
      "activation_833 80\n",
      "activation_836 81\n",
      "block35_2_mixed 82\n",
      "block35_2_conv 83\n",
      "block35_2 84\n",
      "block35_2_ac 85\n",
      "conv2d_840 86\n",
      "batch_normalization_840 87\n",
      "activation_840 88\n",
      "conv2d_838 89\n",
      "conv2d_841 90\n",
      "batch_normalization_838 91\n",
      "batch_normalization_841 92\n",
      "activation_838 93\n",
      "activation_841 94\n",
      "conv2d_837 95\n",
      "conv2d_839 96\n",
      "conv2d_842 97\n",
      "batch_normalization_837 98\n",
      "batch_normalization_839 99\n",
      "batch_normalization_842 100\n",
      "activation_837 101\n",
      "activation_839 102\n",
      "activation_842 103\n",
      "block35_3_mixed 104\n",
      "block35_3_conv 105\n",
      "block35_3 106\n",
      "block35_3_ac 107\n",
      "conv2d_846 108\n",
      "batch_normalization_846 109\n",
      "activation_846 110\n",
      "conv2d_844 111\n",
      "conv2d_847 112\n",
      "batch_normalization_844 113\n",
      "batch_normalization_847 114\n",
      "activation_844 115\n",
      "activation_847 116\n",
      "conv2d_843 117\n",
      "conv2d_845 118\n",
      "conv2d_848 119\n",
      "batch_normalization_843 120\n",
      "batch_normalization_845 121\n",
      "batch_normalization_848 122\n",
      "activation_843 123\n",
      "activation_845 124\n",
      "activation_848 125\n",
      "block35_4_mixed 126\n",
      "block35_4_conv 127\n",
      "block35_4 128\n",
      "block35_4_ac 129\n",
      "conv2d_852 130\n",
      "batch_normalization_852 131\n",
      "activation_852 132\n",
      "conv2d_850 133\n",
      "conv2d_853 134\n",
      "batch_normalization_850 135\n",
      "batch_normalization_853 136\n",
      "activation_850 137\n",
      "activation_853 138\n",
      "conv2d_849 139\n",
      "conv2d_851 140\n",
      "conv2d_854 141\n",
      "batch_normalization_849 142\n",
      "batch_normalization_851 143\n",
      "batch_normalization_854 144\n",
      "activation_849 145\n",
      "activation_851 146\n",
      "activation_854 147\n",
      "block35_5_mixed 148\n",
      "block35_5_conv 149\n",
      "block35_5 150\n",
      "block35_5_ac 151\n",
      "conv2d_858 152\n",
      "batch_normalization_858 153\n",
      "activation_858 154\n",
      "conv2d_856 155\n",
      "conv2d_859 156\n",
      "batch_normalization_856 157\n",
      "batch_normalization_859 158\n",
      "activation_856 159\n",
      "activation_859 160\n",
      "conv2d_855 161\n",
      "conv2d_857 162\n",
      "conv2d_860 163\n",
      "batch_normalization_855 164\n",
      "batch_normalization_857 165\n",
      "batch_normalization_860 166\n",
      "activation_855 167\n",
      "activation_857 168\n",
      "activation_860 169\n",
      "block35_6_mixed 170\n",
      "block35_6_conv 171\n",
      "block35_6 172\n",
      "block35_6_ac 173\n",
      "conv2d_864 174\n",
      "batch_normalization_864 175\n",
      "activation_864 176\n",
      "conv2d_862 177\n",
      "conv2d_865 178\n",
      "batch_normalization_862 179\n",
      "batch_normalization_865 180\n",
      "activation_862 181\n",
      "activation_865 182\n",
      "conv2d_861 183\n",
      "conv2d_863 184\n",
      "conv2d_866 185\n",
      "batch_normalization_861 186\n",
      "batch_normalization_863 187\n",
      "batch_normalization_866 188\n",
      "activation_861 189\n",
      "activation_863 190\n",
      "activation_866 191\n",
      "block35_7_mixed 192\n",
      "block35_7_conv 193\n",
      "block35_7 194\n",
      "block35_7_ac 195\n",
      "conv2d_870 196\n",
      "batch_normalization_870 197\n",
      "activation_870 198\n",
      "conv2d_868 199\n",
      "conv2d_871 200\n",
      "batch_normalization_868 201\n",
      "batch_normalization_871 202\n",
      "activation_868 203\n",
      "activation_871 204\n",
      "conv2d_867 205\n",
      "conv2d_869 206\n",
      "conv2d_872 207\n",
      "batch_normalization_867 208\n",
      "batch_normalization_869 209\n",
      "batch_normalization_872 210\n",
      "activation_867 211\n",
      "activation_869 212\n",
      "activation_872 213\n",
      "block35_8_mixed 214\n",
      "block35_8_conv 215\n",
      "block35_8 216\n",
      "block35_8_ac 217\n",
      "conv2d_876 218\n",
      "batch_normalization_876 219\n",
      "activation_876 220\n",
      "conv2d_874 221\n",
      "conv2d_877 222\n",
      "batch_normalization_874 223\n",
      "batch_normalization_877 224\n",
      "activation_874 225\n",
      "activation_877 226\n",
      "conv2d_873 227\n",
      "conv2d_875 228\n",
      "conv2d_878 229\n",
      "batch_normalization_873 230\n",
      "batch_normalization_875 231\n",
      "batch_normalization_878 232\n",
      "activation_873 233\n",
      "activation_875 234\n",
      "activation_878 235\n",
      "block35_9_mixed 236\n",
      "block35_9_conv 237\n",
      "block35_9 238\n",
      "block35_9_ac 239\n",
      "conv2d_882 240\n",
      "batch_normalization_882 241\n",
      "activation_882 242\n",
      "conv2d_880 243\n",
      "conv2d_883 244\n",
      "batch_normalization_880 245\n",
      "batch_normalization_883 246\n",
      "activation_880 247\n",
      "activation_883 248\n",
      "conv2d_879 249\n",
      "conv2d_881 250\n",
      "conv2d_884 251\n",
      "batch_normalization_879 252\n",
      "batch_normalization_881 253\n",
      "batch_normalization_884 254\n",
      "activation_879 255\n",
      "activation_881 256\n",
      "activation_884 257\n",
      "block35_10_mixed 258\n",
      "block35_10_conv 259\n",
      "block35_10 260\n",
      "block35_10_ac 261\n",
      "conv2d_886 262\n",
      "batch_normalization_886 263\n",
      "activation_886 264\n",
      "conv2d_887 265\n",
      "batch_normalization_887 266\n",
      "activation_887 267\n",
      "conv2d_885 268\n",
      "conv2d_888 269\n",
      "batch_normalization_885 270\n",
      "batch_normalization_888 271\n",
      "activation_885 272\n",
      "activation_888 273\n",
      "max_pooling2d_19 274\n",
      "mixed_6a 275\n",
      "conv2d_890 276\n",
      "batch_normalization_890 277\n",
      "activation_890 278\n",
      "conv2d_891 279\n",
      "batch_normalization_891 280\n",
      "activation_891 281\n",
      "conv2d_889 282\n",
      "conv2d_892 283\n",
      "batch_normalization_889 284\n",
      "batch_normalization_892 285\n",
      "activation_889 286\n",
      "activation_892 287\n",
      "block17_1_mixed 288\n",
      "block17_1_conv 289\n",
      "block17_1 290\n",
      "block17_1_ac 291\n",
      "conv2d_894 292\n",
      "batch_normalization_894 293\n",
      "activation_894 294\n",
      "conv2d_895 295\n",
      "batch_normalization_895 296\n",
      "activation_895 297\n",
      "conv2d_893 298\n",
      "conv2d_896 299\n",
      "batch_normalization_893 300\n",
      "batch_normalization_896 301\n",
      "activation_893 302\n",
      "activation_896 303\n",
      "block17_2_mixed 304\n",
      "block17_2_conv 305\n",
      "block17_2 306\n",
      "block17_2_ac 307\n",
      "conv2d_898 308\n",
      "batch_normalization_898 309\n",
      "activation_898 310\n",
      "conv2d_899 311\n",
      "batch_normalization_899 312\n",
      "activation_899 313\n",
      "conv2d_897 314\n",
      "conv2d_900 315\n",
      "batch_normalization_897 316\n",
      "batch_normalization_900 317\n",
      "activation_897 318\n",
      "activation_900 319\n",
      "block17_3_mixed 320\n",
      "block17_3_conv 321\n",
      "block17_3 322\n",
      "block17_3_ac 323\n",
      "conv2d_902 324\n",
      "batch_normalization_902 325\n",
      "activation_902 326\n",
      "conv2d_903 327\n",
      "batch_normalization_903 328\n",
      "activation_903 329\n",
      "conv2d_901 330\n",
      "conv2d_904 331\n",
      "batch_normalization_901 332\n",
      "batch_normalization_904 333\n",
      "activation_901 334\n",
      "activation_904 335\n",
      "block17_4_mixed 336\n",
      "block17_4_conv 337\n",
      "block17_4 338\n",
      "block17_4_ac 339\n",
      "conv2d_906 340\n",
      "batch_normalization_906 341\n",
      "activation_906 342\n",
      "conv2d_907 343\n",
      "batch_normalization_907 344\n",
      "activation_907 345\n",
      "conv2d_905 346\n",
      "conv2d_908 347\n",
      "batch_normalization_905 348\n",
      "batch_normalization_908 349\n",
      "activation_905 350\n",
      "activation_908 351\n",
      "block17_5_mixed 352\n",
      "block17_5_conv 353\n",
      "block17_5 354\n",
      "block17_5_ac 355\n",
      "conv2d_910 356\n",
      "batch_normalization_910 357\n",
      "activation_910 358\n",
      "conv2d_911 359\n",
      "batch_normalization_911 360\n",
      "activation_911 361\n",
      "conv2d_909 362\n",
      "conv2d_912 363\n",
      "batch_normalization_909 364\n",
      "batch_normalization_912 365\n",
      "activation_909 366\n",
      "activation_912 367\n",
      "block17_6_mixed 368\n",
      "block17_6_conv 369\n",
      "block17_6 370\n",
      "block17_6_ac 371\n",
      "conv2d_914 372\n",
      "batch_normalization_914 373\n",
      "activation_914 374\n",
      "conv2d_915 375\n",
      "batch_normalization_915 376\n",
      "activation_915 377\n",
      "conv2d_913 378\n",
      "conv2d_916 379\n",
      "batch_normalization_913 380\n",
      "batch_normalization_916 381\n",
      "activation_913 382\n",
      "activation_916 383\n",
      "block17_7_mixed 384\n",
      "block17_7_conv 385\n",
      "block17_7 386\n",
      "block17_7_ac 387\n",
      "conv2d_918 388\n",
      "batch_normalization_918 389\n",
      "activation_918 390\n",
      "conv2d_919 391\n",
      "batch_normalization_919 392\n",
      "activation_919 393\n",
      "conv2d_917 394\n",
      "conv2d_920 395\n",
      "batch_normalization_917 396\n",
      "batch_normalization_920 397\n",
      "activation_917 398\n",
      "activation_920 399\n",
      "block17_8_mixed 400\n",
      "block17_8_conv 401\n",
      "block17_8 402\n",
      "block17_8_ac 403\n",
      "conv2d_922 404\n",
      "batch_normalization_922 405\n",
      "activation_922 406\n",
      "conv2d_923 407\n",
      "batch_normalization_923 408\n",
      "activation_923 409\n",
      "conv2d_921 410\n",
      "conv2d_924 411\n",
      "batch_normalization_921 412\n",
      "batch_normalization_924 413\n",
      "activation_921 414\n",
      "activation_924 415\n",
      "block17_9_mixed 416\n",
      "block17_9_conv 417\n",
      "block17_9 418\n",
      "block17_9_ac 419\n",
      "conv2d_926 420\n",
      "batch_normalization_926 421\n",
      "activation_926 422\n",
      "conv2d_927 423\n",
      "batch_normalization_927 424\n",
      "activation_927 425\n",
      "conv2d_925 426\n",
      "conv2d_928 427\n",
      "batch_normalization_925 428\n",
      "batch_normalization_928 429\n",
      "activation_925 430\n",
      "activation_928 431\n",
      "block17_10_mixed 432\n",
      "block17_10_conv 433\n",
      "block17_10 434\n",
      "block17_10_ac 435\n",
      "conv2d_930 436\n",
      "batch_normalization_930 437\n",
      "activation_930 438\n",
      "conv2d_931 439\n",
      "batch_normalization_931 440\n",
      "activation_931 441\n",
      "conv2d_929 442\n",
      "conv2d_932 443\n",
      "batch_normalization_929 444\n",
      "batch_normalization_932 445\n",
      "activation_929 446\n",
      "activation_932 447\n",
      "block17_11_mixed 448\n",
      "block17_11_conv 449\n",
      "block17_11 450\n",
      "block17_11_ac 451\n",
      "conv2d_934 452\n",
      "batch_normalization_934 453\n",
      "activation_934 454\n",
      "conv2d_935 455\n",
      "batch_normalization_935 456\n",
      "activation_935 457\n",
      "conv2d_933 458\n",
      "conv2d_936 459\n",
      "batch_normalization_933 460\n",
      "batch_normalization_936 461\n",
      "activation_933 462\n",
      "activation_936 463\n",
      "block17_12_mixed 464\n",
      "block17_12_conv 465\n",
      "block17_12 466\n",
      "block17_12_ac 467\n",
      "conv2d_938 468\n",
      "batch_normalization_938 469\n",
      "activation_938 470\n",
      "conv2d_939 471\n",
      "batch_normalization_939 472\n",
      "activation_939 473\n",
      "conv2d_937 474\n",
      "conv2d_940 475\n",
      "batch_normalization_937 476\n",
      "batch_normalization_940 477\n",
      "activation_937 478\n",
      "activation_940 479\n",
      "block17_13_mixed 480\n",
      "block17_13_conv 481\n",
      "block17_13 482\n",
      "block17_13_ac 483\n",
      "conv2d_942 484\n",
      "batch_normalization_942 485\n",
      "activation_942 486\n",
      "conv2d_943 487\n",
      "batch_normalization_943 488\n",
      "activation_943 489\n",
      "conv2d_941 490\n",
      "conv2d_944 491\n",
      "batch_normalization_941 492\n",
      "batch_normalization_944 493\n",
      "activation_941 494\n",
      "activation_944 495\n",
      "block17_14_mixed 496\n",
      "block17_14_conv 497\n",
      "block17_14 498\n",
      "block17_14_ac 499\n",
      "conv2d_946 500\n",
      "batch_normalization_946 501\n",
      "activation_946 502\n",
      "conv2d_947 503\n",
      "batch_normalization_947 504\n",
      "activation_947 505\n",
      "conv2d_945 506\n",
      "conv2d_948 507\n",
      "batch_normalization_945 508\n",
      "batch_normalization_948 509\n",
      "activation_945 510\n",
      "activation_948 511\n",
      "block17_15_mixed 512\n",
      "block17_15_conv 513\n",
      "block17_15 514\n",
      "block17_15_ac 515\n",
      "conv2d_950 516\n",
      "batch_normalization_950 517\n",
      "activation_950 518\n",
      "conv2d_951 519\n",
      "batch_normalization_951 520\n",
      "activation_951 521\n",
      "conv2d_949 522\n",
      "conv2d_952 523\n",
      "batch_normalization_949 524\n",
      "batch_normalization_952 525\n",
      "activation_949 526\n",
      "activation_952 527\n",
      "block17_16_mixed 528\n",
      "block17_16_conv 529\n",
      "block17_16 530\n",
      "block17_16_ac 531\n",
      "conv2d_954 532\n",
      "batch_normalization_954 533\n",
      "activation_954 534\n",
      "conv2d_955 535\n",
      "batch_normalization_955 536\n",
      "activation_955 537\n",
      "conv2d_953 538\n",
      "conv2d_956 539\n",
      "batch_normalization_953 540\n",
      "batch_normalization_956 541\n",
      "activation_953 542\n",
      "activation_956 543\n",
      "block17_17_mixed 544\n",
      "block17_17_conv 545\n",
      "block17_17 546\n",
      "block17_17_ac 547\n",
      "conv2d_958 548\n",
      "batch_normalization_958 549\n",
      "activation_958 550\n",
      "conv2d_959 551\n",
      "batch_normalization_959 552\n",
      "activation_959 553\n",
      "conv2d_957 554\n",
      "conv2d_960 555\n",
      "batch_normalization_957 556\n",
      "batch_normalization_960 557\n",
      "activation_957 558\n",
      "activation_960 559\n",
      "block17_18_mixed 560\n",
      "block17_18_conv 561\n",
      "block17_18 562\n",
      "block17_18_ac 563\n",
      "conv2d_962 564\n",
      "batch_normalization_962 565\n",
      "activation_962 566\n",
      "conv2d_963 567\n",
      "batch_normalization_963 568\n",
      "activation_963 569\n",
      "conv2d_961 570\n",
      "conv2d_964 571\n",
      "batch_normalization_961 572\n",
      "batch_normalization_964 573\n",
      "activation_961 574\n",
      "activation_964 575\n",
      "block17_19_mixed 576\n",
      "block17_19_conv 577\n",
      "block17_19 578\n",
      "block17_19_ac 579\n",
      "conv2d_966 580\n",
      "batch_normalization_966 581\n",
      "activation_966 582\n",
      "conv2d_967 583\n",
      "batch_normalization_967 584\n",
      "activation_967 585\n",
      "conv2d_965 586\n",
      "conv2d_968 587\n",
      "batch_normalization_965 588\n",
      "batch_normalization_968 589\n",
      "activation_965 590\n",
      "activation_968 591\n",
      "block17_20_mixed 592\n",
      "block17_20_conv 593\n",
      "block17_20 594\n",
      "block17_20_ac 595\n",
      "conv2d_973 596\n",
      "batch_normalization_973 597\n",
      "activation_973 598\n",
      "conv2d_969 599\n",
      "conv2d_971 600\n",
      "conv2d_974 601\n",
      "batch_normalization_969 602\n",
      "batch_normalization_971 603\n",
      "batch_normalization_974 604\n",
      "activation_969 605\n",
      "activation_971 606\n",
      "activation_974 607\n",
      "conv2d_970 608\n",
      "conv2d_972 609\n",
      "conv2d_975 610\n",
      "batch_normalization_970 611\n",
      "batch_normalization_972 612\n",
      "batch_normalization_975 613\n",
      "activation_970 614\n",
      "activation_972 615\n",
      "activation_975 616\n",
      "max_pooling2d_20 617\n",
      "mixed_7a 618\n",
      "conv2d_977 619\n",
      "batch_normalization_977 620\n",
      "activation_977 621\n",
      "conv2d_978 622\n",
      "batch_normalization_978 623\n",
      "activation_978 624\n",
      "conv2d_976 625\n",
      "conv2d_979 626\n",
      "batch_normalization_976 627\n",
      "batch_normalization_979 628\n",
      "activation_976 629\n",
      "activation_979 630\n",
      "block8_1_mixed 631\n",
      "block8_1_conv 632\n",
      "block8_1 633\n",
      "block8_1_ac 634\n",
      "conv2d_981 635\n",
      "batch_normalization_981 636\n",
      "activation_981 637\n",
      "conv2d_982 638\n",
      "batch_normalization_982 639\n",
      "activation_982 640\n",
      "conv2d_980 641\n",
      "conv2d_983 642\n",
      "batch_normalization_980 643\n",
      "batch_normalization_983 644\n",
      "activation_980 645\n",
      "activation_983 646\n",
      "block8_2_mixed 647\n",
      "block8_2_conv 648\n",
      "block8_2 649\n",
      "block8_2_ac 650\n",
      "conv2d_985 651\n",
      "batch_normalization_985 652\n",
      "activation_985 653\n",
      "conv2d_986 654\n",
      "batch_normalization_986 655\n",
      "activation_986 656\n",
      "conv2d_984 657\n",
      "conv2d_987 658\n",
      "batch_normalization_984 659\n",
      "batch_normalization_987 660\n",
      "activation_984 661\n",
      "activation_987 662\n",
      "block8_3_mixed 663\n",
      "block8_3_conv 664\n",
      "block8_3 665\n",
      "block8_3_ac 666\n",
      "conv2d_989 667\n",
      "batch_normalization_989 668\n",
      "activation_989 669\n",
      "conv2d_990 670\n",
      "batch_normalization_990 671\n",
      "activation_990 672\n",
      "conv2d_988 673\n",
      "conv2d_991 674\n",
      "batch_normalization_988 675\n",
      "batch_normalization_991 676\n",
      "activation_988 677\n",
      "activation_991 678\n",
      "block8_4_mixed 679\n",
      "block8_4_conv 680\n",
      "block8_4 681\n",
      "block8_4_ac 682\n",
      "conv2d_993 683\n",
      "batch_normalization_993 684\n",
      "activation_993 685\n",
      "conv2d_994 686\n",
      "batch_normalization_994 687\n",
      "activation_994 688\n",
      "conv2d_992 689\n",
      "conv2d_995 690\n",
      "batch_normalization_992 691\n",
      "batch_normalization_995 692\n",
      "activation_992 693\n",
      "activation_995 694\n",
      "block8_5_mixed 695\n",
      "block8_5_conv 696\n",
      "block8_5 697\n",
      "block8_5_ac 698\n",
      "conv2d_997 699\n",
      "batch_normalization_997 700\n",
      "activation_997 701\n",
      "conv2d_998 702\n",
      "batch_normalization_998 703\n",
      "activation_998 704\n",
      "conv2d_996 705\n",
      "conv2d_999 706\n",
      "batch_normalization_996 707\n",
      "batch_normalization_999 708\n",
      "activation_996 709\n",
      "activation_999 710\n",
      "block8_6_mixed 711\n",
      "block8_6_conv 712\n",
      "block8_6 713\n",
      "block8_6_ac 714\n",
      "conv2d_1001 715\n",
      "batch_normalization_1001 716\n",
      "activation_1001 717\n",
      "conv2d_1002 718\n",
      "batch_normalization_1002 719\n",
      "activation_1002 720\n",
      "conv2d_1000 721\n",
      "conv2d_1003 722\n",
      "batch_normalization_1000 723\n",
      "batch_normalization_1003 724\n",
      "activation_1000 725\n",
      "activation_1003 726\n",
      "block8_7_mixed 727\n",
      "block8_7_conv 728\n",
      "block8_7 729\n",
      "block8_7_ac 730\n",
      "conv2d_1005 731\n",
      "batch_normalization_1005 732\n",
      "activation_1005 733\n",
      "conv2d_1006 734\n",
      "batch_normalization_1006 735\n",
      "activation_1006 736\n",
      "conv2d_1004 737\n",
      "conv2d_1007 738\n",
      "batch_normalization_1004 739\n",
      "batch_normalization_1007 740\n",
      "activation_1004 741\n",
      "activation_1007 742\n",
      "block8_8_mixed 743\n",
      "block8_8_conv 744\n",
      "block8_8 745\n",
      "block8_8_ac 746\n",
      "conv2d_1009 747\n",
      "batch_normalization_1009 748\n",
      "activation_1009 749\n",
      "conv2d_1010 750\n",
      "batch_normalization_1010 751\n",
      "activation_1010 752\n",
      "conv2d_1008 753\n",
      "conv2d_1011 754\n",
      "batch_normalization_1008 755\n",
      "batch_normalization_1011 756\n",
      "activation_1008 757\n",
      "activation_1011 758\n",
      "block8_9_mixed 759\n",
      "block8_9_conv 760\n",
      "block8_9 761\n",
      "block8_9_ac 762\n",
      "conv2d_1013 763\n",
      "batch_normalization_1013 764\n",
      "activation_1013 765\n",
      "conv2d_1014 766\n",
      "batch_normalization_1014 767\n",
      "activation_1014 768\n",
      "conv2d_1012 769\n",
      "conv2d_1015 770\n",
      "batch_normalization_1012 771\n",
      "batch_normalization_1015 772\n",
      "activation_1012 773\n",
      "activation_1015 774\n",
      "block8_10_mixed 775\n",
      "block8_10_conv 776\n",
      "block8_10 777\n",
      "conv_7b 778\n",
      "conv_7b_bn 779\n",
      "conv_7b_ac 780\n",
      "global_average_pooling2d_5 781\n",
      "dropout_5 782\n",
      "dense_5 783\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    print(model.layers[i].name, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放开靠后部分网络层进行 fine tune 微调模型\n",
    "for layer in model.layers[770:]:\t# total 783 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 172s 9ms/step - loss: 0.0956 - acc: 0.9648 - val_loss: 0.0413 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.05668 to 0.04128, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0940 - acc: 0.9646 - val_loss: 0.0413 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04128\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.0895 - acc: 0.9682 - val_loss: 0.0422 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04128\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0869 - acc: 0.9677 - val_loss: 0.0402 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04128 to 0.04017, saving model to weights_checkpint_060_770.hdf5\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.0886 - acc: 0.9669 - val_loss: 0.0426 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04017\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.0865 - acc: 0.9674 - val_loss: 0.0424 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04017\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.0856 - acc: 0.9687 - val_loss: 0.0433 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04017\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.0793 - acc: 0.9708 - val_loss: 0.0414 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6e5cff0f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, \t# first: 16\n",
    "\tepochs=8, \t# first: 5 \n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelInceptionResNet_transfer_Doup060_770layer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取测试图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:28<00:00, 438.52it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(12500)):\n",
    "    j = i+1\n",
    "    X_test[i] = cv2.resize(cv2.imread('test/%d.jpg' % j), (img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 154s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for i in range(12500):\n",
    "\tdf.set_value(i, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred_changePredictCsvMethond_Drop060_770layer.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
