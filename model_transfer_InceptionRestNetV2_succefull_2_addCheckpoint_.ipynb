{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"all.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"train.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref = zipfile.ZipFile(\"test.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GlobalAveragePooling2D, Dropout, Lambda\n",
    "from keras.applications import inception_resnet_v2\n",
    "# from keras.preprocessing.image import *\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [03:56<00:00, 52.81it/s]\n"
     ]
    }
   ],
   "source": [
    "n = 25000\n",
    "img_height, img_width = 299, 299\n",
    "X = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, 1), dtype=np.uint8)\n",
    "\n",
    "for i in tqdm(range(12500)): # n/2\n",
    "    X[i] = cv2.resize(cv2.imread('train/cat.%d.jpg' % i), (img_height, img_width))\n",
    "    X[i+12500] = cv2.resize(cv2.imread('train/dog.%d.jpg' % i), (img_height, img_width)) # n/2\n",
    "\n",
    "# 打上标签\n",
    "y[12500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_input_tensor = Input((img_height, img_width, 3))\n",
    "# x_input_tensor = inception_resnet_v2.preprocess_input(i_input_tensor)\t# 错误的处理\n",
    "x_input_tensor = Lambda(inception_resnet_v2.preprocess_input)(i_input_tensor) # 建立模型预处理\n",
    "\n",
    "# 搭建基础模型\n",
    "base_model = inception_resnet_v2.InceptionResNetV2(input_tensor=x_input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "范例：不运行\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "如果验证损失下降， 那么在每个训练轮之后保存模型。\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_checkpint.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 锁定所有神经网络层\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "i_output = GlobalAveragePooling2D()(base_model.output)\n",
    "i_output = Dropout(0.5)(i_output) # 0.25 比 0.5 好\n",
    "i_predictions = Dense(1, activation='sigmoid')(i_output)\n",
    "model = Model(base_model.input, i_predictions)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 156s 8ms/step - loss: 0.1433 - acc: 0.9508 - val_loss: 0.0386 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03454\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 146s 7ms/step - loss: 0.0923 - acc: 0.9651 - val_loss: 0.0425 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03454\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0846 - acc: 0.9676 - val_loss: 0.0325 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03454 to 0.03248, saving model to weights_checkpint.hdf5\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0766 - acc: 0.9707 - val_loss: 0.0396 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03248\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0804 - acc: 0.9680 - val_loss: 0.0218 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03248 to 0.02176, saving model to weights_checkpint.hdf5\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 146s 7ms/step - loss: 0.0828 - acc: 0.9693 - val_loss: 0.0403 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02176\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0767 - acc: 0.9705 - val_loss: 0.0398 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02176\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 147s 7ms/step - loss: 0.0771 - acc: 0.9710 - val_loss: 0.0551 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf7802cfd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20,     # defalt 16\n",
    "\tepochs=8, \t# defalt 5\n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 0\n",
      "lambda_2 1\n",
      "conv2d_204 2\n",
      "batch_normalization_204 3\n",
      "activation_204 4\n",
      "conv2d_205 5\n",
      "batch_normalization_205 6\n",
      "activation_205 7\n",
      "conv2d_206 8\n",
      "batch_normalization_206 9\n",
      "activation_206 10\n",
      "max_pooling2d_5 11\n",
      "conv2d_207 12\n",
      "batch_normalization_207 13\n",
      "activation_207 14\n",
      "conv2d_208 15\n",
      "batch_normalization_208 16\n",
      "activation_208 17\n",
      "max_pooling2d_6 18\n",
      "conv2d_212 19\n",
      "batch_normalization_212 20\n",
      "activation_212 21\n",
      "conv2d_210 22\n",
      "conv2d_213 23\n",
      "batch_normalization_210 24\n",
      "batch_normalization_213 25\n",
      "activation_210 26\n",
      "activation_213 27\n",
      "average_pooling2d_2 28\n",
      "conv2d_209 29\n",
      "conv2d_211 30\n",
      "conv2d_214 31\n",
      "conv2d_215 32\n",
      "batch_normalization_209 33\n",
      "batch_normalization_211 34\n",
      "batch_normalization_214 35\n",
      "batch_normalization_215 36\n",
      "activation_209 37\n",
      "activation_211 38\n",
      "activation_214 39\n",
      "activation_215 40\n",
      "mixed_5b 41\n",
      "conv2d_219 42\n",
      "batch_normalization_219 43\n",
      "activation_219 44\n",
      "conv2d_217 45\n",
      "conv2d_220 46\n",
      "batch_normalization_217 47\n",
      "batch_normalization_220 48\n",
      "activation_217 49\n",
      "activation_220 50\n",
      "conv2d_216 51\n",
      "conv2d_218 52\n",
      "conv2d_221 53\n",
      "batch_normalization_216 54\n",
      "batch_normalization_218 55\n",
      "batch_normalization_221 56\n",
      "activation_216 57\n",
      "activation_218 58\n",
      "activation_221 59\n",
      "block35_1_mixed 60\n",
      "block35_1_conv 61\n",
      "block35_1 62\n",
      "block35_1_ac 63\n",
      "conv2d_225 64\n",
      "batch_normalization_225 65\n",
      "activation_225 66\n",
      "conv2d_223 67\n",
      "conv2d_226 68\n",
      "batch_normalization_223 69\n",
      "batch_normalization_226 70\n",
      "activation_223 71\n",
      "activation_226 72\n",
      "conv2d_222 73\n",
      "conv2d_224 74\n",
      "conv2d_227 75\n",
      "batch_normalization_222 76\n",
      "batch_normalization_224 77\n",
      "batch_normalization_227 78\n",
      "activation_222 79\n",
      "activation_224 80\n",
      "activation_227 81\n",
      "block35_2_mixed 82\n",
      "block35_2_conv 83\n",
      "block35_2 84\n",
      "block35_2_ac 85\n",
      "conv2d_231 86\n",
      "batch_normalization_231 87\n",
      "activation_231 88\n",
      "conv2d_229 89\n",
      "conv2d_232 90\n",
      "batch_normalization_229 91\n",
      "batch_normalization_232 92\n",
      "activation_229 93\n",
      "activation_232 94\n",
      "conv2d_228 95\n",
      "conv2d_230 96\n",
      "conv2d_233 97\n",
      "batch_normalization_228 98\n",
      "batch_normalization_230 99\n",
      "batch_normalization_233 100\n",
      "activation_228 101\n",
      "activation_230 102\n",
      "activation_233 103\n",
      "block35_3_mixed 104\n",
      "block35_3_conv 105\n",
      "block35_3 106\n",
      "block35_3_ac 107\n",
      "conv2d_237 108\n",
      "batch_normalization_237 109\n",
      "activation_237 110\n",
      "conv2d_235 111\n",
      "conv2d_238 112\n",
      "batch_normalization_235 113\n",
      "batch_normalization_238 114\n",
      "activation_235 115\n",
      "activation_238 116\n",
      "conv2d_234 117\n",
      "conv2d_236 118\n",
      "conv2d_239 119\n",
      "batch_normalization_234 120\n",
      "batch_normalization_236 121\n",
      "batch_normalization_239 122\n",
      "activation_234 123\n",
      "activation_236 124\n",
      "activation_239 125\n",
      "block35_4_mixed 126\n",
      "block35_4_conv 127\n",
      "block35_4 128\n",
      "block35_4_ac 129\n",
      "conv2d_243 130\n",
      "batch_normalization_243 131\n",
      "activation_243 132\n",
      "conv2d_241 133\n",
      "conv2d_244 134\n",
      "batch_normalization_241 135\n",
      "batch_normalization_244 136\n",
      "activation_241 137\n",
      "activation_244 138\n",
      "conv2d_240 139\n",
      "conv2d_242 140\n",
      "conv2d_245 141\n",
      "batch_normalization_240 142\n",
      "batch_normalization_242 143\n",
      "batch_normalization_245 144\n",
      "activation_240 145\n",
      "activation_242 146\n",
      "activation_245 147\n",
      "block35_5_mixed 148\n",
      "block35_5_conv 149\n",
      "block35_5 150\n",
      "block35_5_ac 151\n",
      "conv2d_249 152\n",
      "batch_normalization_249 153\n",
      "activation_249 154\n",
      "conv2d_247 155\n",
      "conv2d_250 156\n",
      "batch_normalization_247 157\n",
      "batch_normalization_250 158\n",
      "activation_247 159\n",
      "activation_250 160\n",
      "conv2d_246 161\n",
      "conv2d_248 162\n",
      "conv2d_251 163\n",
      "batch_normalization_246 164\n",
      "batch_normalization_248 165\n",
      "batch_normalization_251 166\n",
      "activation_246 167\n",
      "activation_248 168\n",
      "activation_251 169\n",
      "block35_6_mixed 170\n",
      "block35_6_conv 171\n",
      "block35_6 172\n",
      "block35_6_ac 173\n",
      "conv2d_255 174\n",
      "batch_normalization_255 175\n",
      "activation_255 176\n",
      "conv2d_253 177\n",
      "conv2d_256 178\n",
      "batch_normalization_253 179\n",
      "batch_normalization_256 180\n",
      "activation_253 181\n",
      "activation_256 182\n",
      "conv2d_252 183\n",
      "conv2d_254 184\n",
      "conv2d_257 185\n",
      "batch_normalization_252 186\n",
      "batch_normalization_254 187\n",
      "batch_normalization_257 188\n",
      "activation_252 189\n",
      "activation_254 190\n",
      "activation_257 191\n",
      "block35_7_mixed 192\n",
      "block35_7_conv 193\n",
      "block35_7 194\n",
      "block35_7_ac 195\n",
      "conv2d_261 196\n",
      "batch_normalization_261 197\n",
      "activation_261 198\n",
      "conv2d_259 199\n",
      "conv2d_262 200\n",
      "batch_normalization_259 201\n",
      "batch_normalization_262 202\n",
      "activation_259 203\n",
      "activation_262 204\n",
      "conv2d_258 205\n",
      "conv2d_260 206\n",
      "conv2d_263 207\n",
      "batch_normalization_258 208\n",
      "batch_normalization_260 209\n",
      "batch_normalization_263 210\n",
      "activation_258 211\n",
      "activation_260 212\n",
      "activation_263 213\n",
      "block35_8_mixed 214\n",
      "block35_8_conv 215\n",
      "block35_8 216\n",
      "block35_8_ac 217\n",
      "conv2d_267 218\n",
      "batch_normalization_267 219\n",
      "activation_267 220\n",
      "conv2d_265 221\n",
      "conv2d_268 222\n",
      "batch_normalization_265 223\n",
      "batch_normalization_268 224\n",
      "activation_265 225\n",
      "activation_268 226\n",
      "conv2d_264 227\n",
      "conv2d_266 228\n",
      "conv2d_269 229\n",
      "batch_normalization_264 230\n",
      "batch_normalization_266 231\n",
      "batch_normalization_269 232\n",
      "activation_264 233\n",
      "activation_266 234\n",
      "activation_269 235\n",
      "block35_9_mixed 236\n",
      "block35_9_conv 237\n",
      "block35_9 238\n",
      "block35_9_ac 239\n",
      "conv2d_273 240\n",
      "batch_normalization_273 241\n",
      "activation_273 242\n",
      "conv2d_271 243\n",
      "conv2d_274 244\n",
      "batch_normalization_271 245\n",
      "batch_normalization_274 246\n",
      "activation_271 247\n",
      "activation_274 248\n",
      "conv2d_270 249\n",
      "conv2d_272 250\n",
      "conv2d_275 251\n",
      "batch_normalization_270 252\n",
      "batch_normalization_272 253\n",
      "batch_normalization_275 254\n",
      "activation_270 255\n",
      "activation_272 256\n",
      "activation_275 257\n",
      "block35_10_mixed 258\n",
      "block35_10_conv 259\n",
      "block35_10 260\n",
      "block35_10_ac 261\n",
      "conv2d_277 262\n",
      "batch_normalization_277 263\n",
      "activation_277 264\n",
      "conv2d_278 265\n",
      "batch_normalization_278 266\n",
      "activation_278 267\n",
      "conv2d_276 268\n",
      "conv2d_279 269\n",
      "batch_normalization_276 270\n",
      "batch_normalization_279 271\n",
      "activation_276 272\n",
      "activation_279 273\n",
      "max_pooling2d_7 274\n",
      "mixed_6a 275\n",
      "conv2d_281 276\n",
      "batch_normalization_281 277\n",
      "activation_281 278\n",
      "conv2d_282 279\n",
      "batch_normalization_282 280\n",
      "activation_282 281\n",
      "conv2d_280 282\n",
      "conv2d_283 283\n",
      "batch_normalization_280 284\n",
      "batch_normalization_283 285\n",
      "activation_280 286\n",
      "activation_283 287\n",
      "block17_1_mixed 288\n",
      "block17_1_conv 289\n",
      "block17_1 290\n",
      "block17_1_ac 291\n",
      "conv2d_285 292\n",
      "batch_normalization_285 293\n",
      "activation_285 294\n",
      "conv2d_286 295\n",
      "batch_normalization_286 296\n",
      "activation_286 297\n",
      "conv2d_284 298\n",
      "conv2d_287 299\n",
      "batch_normalization_284 300\n",
      "batch_normalization_287 301\n",
      "activation_284 302\n",
      "activation_287 303\n",
      "block17_2_mixed 304\n",
      "block17_2_conv 305\n",
      "block17_2 306\n",
      "block17_2_ac 307\n",
      "conv2d_289 308\n",
      "batch_normalization_289 309\n",
      "activation_289 310\n",
      "conv2d_290 311\n",
      "batch_normalization_290 312\n",
      "activation_290 313\n",
      "conv2d_288 314\n",
      "conv2d_291 315\n",
      "batch_normalization_288 316\n",
      "batch_normalization_291 317\n",
      "activation_288 318\n",
      "activation_291 319\n",
      "block17_3_mixed 320\n",
      "block17_3_conv 321\n",
      "block17_3 322\n",
      "block17_3_ac 323\n",
      "conv2d_293 324\n",
      "batch_normalization_293 325\n",
      "activation_293 326\n",
      "conv2d_294 327\n",
      "batch_normalization_294 328\n",
      "activation_294 329\n",
      "conv2d_292 330\n",
      "conv2d_295 331\n",
      "batch_normalization_292 332\n",
      "batch_normalization_295 333\n",
      "activation_292 334\n",
      "activation_295 335\n",
      "block17_4_mixed 336\n",
      "block17_4_conv 337\n",
      "block17_4 338\n",
      "block17_4_ac 339\n",
      "conv2d_297 340\n",
      "batch_normalization_297 341\n",
      "activation_297 342\n",
      "conv2d_298 343\n",
      "batch_normalization_298 344\n",
      "activation_298 345\n",
      "conv2d_296 346\n",
      "conv2d_299 347\n",
      "batch_normalization_296 348\n",
      "batch_normalization_299 349\n",
      "activation_296 350\n",
      "activation_299 351\n",
      "block17_5_mixed 352\n",
      "block17_5_conv 353\n",
      "block17_5 354\n",
      "block17_5_ac 355\n",
      "conv2d_301 356\n",
      "batch_normalization_301 357\n",
      "activation_301 358\n",
      "conv2d_302 359\n",
      "batch_normalization_302 360\n",
      "activation_302 361\n",
      "conv2d_300 362\n",
      "conv2d_303 363\n",
      "batch_normalization_300 364\n",
      "batch_normalization_303 365\n",
      "activation_300 366\n",
      "activation_303 367\n",
      "block17_6_mixed 368\n",
      "block17_6_conv 369\n",
      "block17_6 370\n",
      "block17_6_ac 371\n",
      "conv2d_305 372\n",
      "batch_normalization_305 373\n",
      "activation_305 374\n",
      "conv2d_306 375\n",
      "batch_normalization_306 376\n",
      "activation_306 377\n",
      "conv2d_304 378\n",
      "conv2d_307 379\n",
      "batch_normalization_304 380\n",
      "batch_normalization_307 381\n",
      "activation_304 382\n",
      "activation_307 383\n",
      "block17_7_mixed 384\n",
      "block17_7_conv 385\n",
      "block17_7 386\n",
      "block17_7_ac 387\n",
      "conv2d_309 388\n",
      "batch_normalization_309 389\n",
      "activation_309 390\n",
      "conv2d_310 391\n",
      "batch_normalization_310 392\n",
      "activation_310 393\n",
      "conv2d_308 394\n",
      "conv2d_311 395\n",
      "batch_normalization_308 396\n",
      "batch_normalization_311 397\n",
      "activation_308 398\n",
      "activation_311 399\n",
      "block17_8_mixed 400\n",
      "block17_8_conv 401\n",
      "block17_8 402\n",
      "block17_8_ac 403\n",
      "conv2d_313 404\n",
      "batch_normalization_313 405\n",
      "activation_313 406\n",
      "conv2d_314 407\n",
      "batch_normalization_314 408\n",
      "activation_314 409\n",
      "conv2d_312 410\n",
      "conv2d_315 411\n",
      "batch_normalization_312 412\n",
      "batch_normalization_315 413\n",
      "activation_312 414\n",
      "activation_315 415\n",
      "block17_9_mixed 416\n",
      "block17_9_conv 417\n",
      "block17_9 418\n",
      "block17_9_ac 419\n",
      "conv2d_317 420\n",
      "batch_normalization_317 421\n",
      "activation_317 422\n",
      "conv2d_318 423\n",
      "batch_normalization_318 424\n",
      "activation_318 425\n",
      "conv2d_316 426\n",
      "conv2d_319 427\n",
      "batch_normalization_316 428\n",
      "batch_normalization_319 429\n",
      "activation_316 430\n",
      "activation_319 431\n",
      "block17_10_mixed 432\n",
      "block17_10_conv 433\n",
      "block17_10 434\n",
      "block17_10_ac 435\n",
      "conv2d_321 436\n",
      "batch_normalization_321 437\n",
      "activation_321 438\n",
      "conv2d_322 439\n",
      "batch_normalization_322 440\n",
      "activation_322 441\n",
      "conv2d_320 442\n",
      "conv2d_323 443\n",
      "batch_normalization_320 444\n",
      "batch_normalization_323 445\n",
      "activation_320 446\n",
      "activation_323 447\n",
      "block17_11_mixed 448\n",
      "block17_11_conv 449\n",
      "block17_11 450\n",
      "block17_11_ac 451\n",
      "conv2d_325 452\n",
      "batch_normalization_325 453\n",
      "activation_325 454\n",
      "conv2d_326 455\n",
      "batch_normalization_326 456\n",
      "activation_326 457\n",
      "conv2d_324 458\n",
      "conv2d_327 459\n",
      "batch_normalization_324 460\n",
      "batch_normalization_327 461\n",
      "activation_324 462\n",
      "activation_327 463\n",
      "block17_12_mixed 464\n",
      "block17_12_conv 465\n",
      "block17_12 466\n",
      "block17_12_ac 467\n",
      "conv2d_329 468\n",
      "batch_normalization_329 469\n",
      "activation_329 470\n",
      "conv2d_330 471\n",
      "batch_normalization_330 472\n",
      "activation_330 473\n",
      "conv2d_328 474\n",
      "conv2d_331 475\n",
      "batch_normalization_328 476\n",
      "batch_normalization_331 477\n",
      "activation_328 478\n",
      "activation_331 479\n",
      "block17_13_mixed 480\n",
      "block17_13_conv 481\n",
      "block17_13 482\n",
      "block17_13_ac 483\n",
      "conv2d_333 484\n",
      "batch_normalization_333 485\n",
      "activation_333 486\n",
      "conv2d_334 487\n",
      "batch_normalization_334 488\n",
      "activation_334 489\n",
      "conv2d_332 490\n",
      "conv2d_335 491\n",
      "batch_normalization_332 492\n",
      "batch_normalization_335 493\n",
      "activation_332 494\n",
      "activation_335 495\n",
      "block17_14_mixed 496\n",
      "block17_14_conv 497\n",
      "block17_14 498\n",
      "block17_14_ac 499\n",
      "conv2d_337 500\n",
      "batch_normalization_337 501\n",
      "activation_337 502\n",
      "conv2d_338 503\n",
      "batch_normalization_338 504\n",
      "activation_338 505\n",
      "conv2d_336 506\n",
      "conv2d_339 507\n",
      "batch_normalization_336 508\n",
      "batch_normalization_339 509\n",
      "activation_336 510\n",
      "activation_339 511\n",
      "block17_15_mixed 512\n",
      "block17_15_conv 513\n",
      "block17_15 514\n",
      "block17_15_ac 515\n",
      "conv2d_341 516\n",
      "batch_normalization_341 517\n",
      "activation_341 518\n",
      "conv2d_342 519\n",
      "batch_normalization_342 520\n",
      "activation_342 521\n",
      "conv2d_340 522\n",
      "conv2d_343 523\n",
      "batch_normalization_340 524\n",
      "batch_normalization_343 525\n",
      "activation_340 526\n",
      "activation_343 527\n",
      "block17_16_mixed 528\n",
      "block17_16_conv 529\n",
      "block17_16 530\n",
      "block17_16_ac 531\n",
      "conv2d_345 532\n",
      "batch_normalization_345 533\n",
      "activation_345 534\n",
      "conv2d_346 535\n",
      "batch_normalization_346 536\n",
      "activation_346 537\n",
      "conv2d_344 538\n",
      "conv2d_347 539\n",
      "batch_normalization_344 540\n",
      "batch_normalization_347 541\n",
      "activation_344 542\n",
      "activation_347 543\n",
      "block17_17_mixed 544\n",
      "block17_17_conv 545\n",
      "block17_17 546\n",
      "block17_17_ac 547\n",
      "conv2d_349 548\n",
      "batch_normalization_349 549\n",
      "activation_349 550\n",
      "conv2d_350 551\n",
      "batch_normalization_350 552\n",
      "activation_350 553\n",
      "conv2d_348 554\n",
      "conv2d_351 555\n",
      "batch_normalization_348 556\n",
      "batch_normalization_351 557\n",
      "activation_348 558\n",
      "activation_351 559\n",
      "block17_18_mixed 560\n",
      "block17_18_conv 561\n",
      "block17_18 562\n",
      "block17_18_ac 563\n",
      "conv2d_353 564\n",
      "batch_normalization_353 565\n",
      "activation_353 566\n",
      "conv2d_354 567\n",
      "batch_normalization_354 568\n",
      "activation_354 569\n",
      "conv2d_352 570\n",
      "conv2d_355 571\n",
      "batch_normalization_352 572\n",
      "batch_normalization_355 573\n",
      "activation_352 574\n",
      "activation_355 575\n",
      "block17_19_mixed 576\n",
      "block17_19_conv 577\n",
      "block17_19 578\n",
      "block17_19_ac 579\n",
      "conv2d_357 580\n",
      "batch_normalization_357 581\n",
      "activation_357 582\n",
      "conv2d_358 583\n",
      "batch_normalization_358 584\n",
      "activation_358 585\n",
      "conv2d_356 586\n",
      "conv2d_359 587\n",
      "batch_normalization_356 588\n",
      "batch_normalization_359 589\n",
      "activation_356 590\n",
      "activation_359 591\n",
      "block17_20_mixed 592\n",
      "block17_20_conv 593\n",
      "block17_20 594\n",
      "block17_20_ac 595\n",
      "conv2d_364 596\n",
      "batch_normalization_364 597\n",
      "activation_364 598\n",
      "conv2d_360 599\n",
      "conv2d_362 600\n",
      "conv2d_365 601\n",
      "batch_normalization_360 602\n",
      "batch_normalization_362 603\n",
      "batch_normalization_365 604\n",
      "activation_360 605\n",
      "activation_362 606\n",
      "activation_365 607\n",
      "conv2d_361 608\n",
      "conv2d_363 609\n",
      "conv2d_366 610\n",
      "batch_normalization_361 611\n",
      "batch_normalization_363 612\n",
      "batch_normalization_366 613\n",
      "activation_361 614\n",
      "activation_363 615\n",
      "activation_366 616\n",
      "max_pooling2d_8 617\n",
      "mixed_7a 618\n",
      "conv2d_368 619\n",
      "batch_normalization_368 620\n",
      "activation_368 621\n",
      "conv2d_369 622\n",
      "batch_normalization_369 623\n",
      "activation_369 624\n",
      "conv2d_367 625\n",
      "conv2d_370 626\n",
      "batch_normalization_367 627\n",
      "batch_normalization_370 628\n",
      "activation_367 629\n",
      "activation_370 630\n",
      "block8_1_mixed 631\n",
      "block8_1_conv 632\n",
      "block8_1 633\n",
      "block8_1_ac 634\n",
      "conv2d_372 635\n",
      "batch_normalization_372 636\n",
      "activation_372 637\n",
      "conv2d_373 638\n",
      "batch_normalization_373 639\n",
      "activation_373 640\n",
      "conv2d_371 641\n",
      "conv2d_374 642\n",
      "batch_normalization_371 643\n",
      "batch_normalization_374 644\n",
      "activation_371 645\n",
      "activation_374 646\n",
      "block8_2_mixed 647\n",
      "block8_2_conv 648\n",
      "block8_2 649\n",
      "block8_2_ac 650\n",
      "conv2d_376 651\n",
      "batch_normalization_376 652\n",
      "activation_376 653\n",
      "conv2d_377 654\n",
      "batch_normalization_377 655\n",
      "activation_377 656\n",
      "conv2d_375 657\n",
      "conv2d_378 658\n",
      "batch_normalization_375 659\n",
      "batch_normalization_378 660\n",
      "activation_375 661\n",
      "activation_378 662\n",
      "block8_3_mixed 663\n",
      "block8_3_conv 664\n",
      "block8_3 665\n",
      "block8_3_ac 666\n",
      "conv2d_380 667\n",
      "batch_normalization_380 668\n",
      "activation_380 669\n",
      "conv2d_381 670\n",
      "batch_normalization_381 671\n",
      "activation_381 672\n",
      "conv2d_379 673\n",
      "conv2d_382 674\n",
      "batch_normalization_379 675\n",
      "batch_normalization_382 676\n",
      "activation_379 677\n",
      "activation_382 678\n",
      "block8_4_mixed 679\n",
      "block8_4_conv 680\n",
      "block8_4 681\n",
      "block8_4_ac 682\n",
      "conv2d_384 683\n",
      "batch_normalization_384 684\n",
      "activation_384 685\n",
      "conv2d_385 686\n",
      "batch_normalization_385 687\n",
      "activation_385 688\n",
      "conv2d_383 689\n",
      "conv2d_386 690\n",
      "batch_normalization_383 691\n",
      "batch_normalization_386 692\n",
      "activation_383 693\n",
      "activation_386 694\n",
      "block8_5_mixed 695\n",
      "block8_5_conv 696\n",
      "block8_5 697\n",
      "block8_5_ac 698\n",
      "conv2d_388 699\n",
      "batch_normalization_388 700\n",
      "activation_388 701\n",
      "conv2d_389 702\n",
      "batch_normalization_389 703\n",
      "activation_389 704\n",
      "conv2d_387 705\n",
      "conv2d_390 706\n",
      "batch_normalization_387 707\n",
      "batch_normalization_390 708\n",
      "activation_387 709\n",
      "activation_390 710\n",
      "block8_6_mixed 711\n",
      "block8_6_conv 712\n",
      "block8_6 713\n",
      "block8_6_ac 714\n",
      "conv2d_392 715\n",
      "batch_normalization_392 716\n",
      "activation_392 717\n",
      "conv2d_393 718\n",
      "batch_normalization_393 719\n",
      "activation_393 720\n",
      "conv2d_391 721\n",
      "conv2d_394 722\n",
      "batch_normalization_391 723\n",
      "batch_normalization_394 724\n",
      "activation_391 725\n",
      "activation_394 726\n",
      "block8_7_mixed 727\n",
      "block8_7_conv 728\n",
      "block8_7 729\n",
      "block8_7_ac 730\n",
      "conv2d_396 731\n",
      "batch_normalization_396 732\n",
      "activation_396 733\n",
      "conv2d_397 734\n",
      "batch_normalization_397 735\n",
      "activation_397 736\n",
      "conv2d_395 737\n",
      "conv2d_398 738\n",
      "batch_normalization_395 739\n",
      "batch_normalization_398 740\n",
      "activation_395 741\n",
      "activation_398 742\n",
      "block8_8_mixed 743\n",
      "block8_8_conv 744\n",
      "block8_8 745\n",
      "block8_8_ac 746\n",
      "conv2d_400 747\n",
      "batch_normalization_400 748\n",
      "activation_400 749\n",
      "conv2d_401 750\n",
      "batch_normalization_401 751\n",
      "activation_401 752\n",
      "conv2d_399 753\n",
      "conv2d_402 754\n",
      "batch_normalization_399 755\n",
      "batch_normalization_402 756\n",
      "activation_399 757\n",
      "activation_402 758\n",
      "block8_9_mixed 759\n",
      "block8_9_conv 760\n",
      "block8_9 761\n",
      "block8_9_ac 762\n",
      "conv2d_404 763\n",
      "batch_normalization_404 764\n",
      "activation_404 765\n",
      "conv2d_405 766\n",
      "batch_normalization_405 767\n",
      "activation_405 768\n",
      "conv2d_403 769\n",
      "conv2d_406 770\n",
      "batch_normalization_403 771\n",
      "batch_normalization_406 772\n",
      "activation_403 773\n",
      "activation_406 774\n",
      "block8_10_mixed 775\n",
      "block8_10_conv 776\n",
      "block8_10 777\n",
      "conv_7b 778\n",
      "conv_7b_bn 779\n",
      "conv_7b_ac 780\n",
      "global_average_pooling2d_2 781\n",
      "dropout_2 782\n",
      "dense_2 783\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    print(model.layers[i].name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放开靠后部分网络层进行 fine tune 微调模型\n",
    "for layer in model.layers[770:]:\t# total 783 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 160s 8ms/step - loss: 0.0760 - acc: 0.9726 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.02176\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0742 - acc: 0.9722 - val_loss: 0.0372 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.02176\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0731 - acc: 0.9720 - val_loss: 0.0375 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02176\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0723 - acc: 0.9732 - val_loss: 0.0368 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02176\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0737 - acc: 0.9725 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02176\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0704 - acc: 0.9737 - val_loss: 0.0358 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02176\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0666 - acc: 0.9743 - val_loss: 0.0367 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02176\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.0676 - acc: 0.9760 - val_loss: 0.0351 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf646018d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, \t# first: 16\n",
    "\tepochs=8, \t# first: 5 \n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelInceptionResNet_transfer_Doup050_770layer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取测试图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:28<00:00, 443.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(12500)):\n",
    "    j = i+1\n",
    "    X_test[i] = cv2.resize(cv2.imread('test/%d.jpg' % j), (img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 151s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for i in range(12500):\n",
    "\tdf.set_value(i, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred_changePredictCsvMethond_Drop050_770layer.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
