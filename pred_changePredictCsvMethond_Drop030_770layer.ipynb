{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"all.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"train.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref = zipfile.ZipFile(\"test.zip\", 'r')\n",
    "zip_ref.extractall(\".\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GlobalAveragePooling2D, Dropout, Lambda\n",
    "from keras.applications import inception_resnet_v2\n",
    "# from keras.preprocessing.image import *\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:57<00:00, 218.75it/s]\n"
     ]
    }
   ],
   "source": [
    "n = 25000\n",
    "img_height, img_width = 299, 299\n",
    "X = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, 1), dtype=np.uint8)\n",
    "\n",
    "for i in tqdm(range(12500)): # n/2\n",
    "    X[i] = cv2.resize(cv2.imread('train/cat.%d.jpg' % i), (img_height, img_width))\n",
    "    X[i+12500] = cv2.resize(cv2.imread('train/dog.%d.jpg' % i), (img_height, img_width)) # n/2\n",
    "\n",
    "# 打上标签\n",
    "y[12500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_input_tensor = Input((img_height, img_width, 3))\n",
    "# x_input_tensor = inception_resnet_v2.preprocess_input(i_input_tensor)\t# 错误的处理\n",
    "x_input_tensor = Lambda(inception_resnet_v2.preprocess_input)(i_input_tensor) # 建立模型预处理\n",
    "\n",
    "# 搭建基础模型\n",
    "base_model = inception_resnet_v2.InceptionResNetV2(input_tensor=x_input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "范例：不运行\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "如果验证损失下降， 那么在每个训练轮之后保存模型。\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_checkpint_030_770.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 锁定所有神经网络层\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "i_output = GlobalAveragePooling2D()(base_model.output)\n",
    "i_output = Dropout(0.3)(i_output) # 0.25 比 0.5 好\n",
    "i_predictions = Dense(1, activation='sigmoid')(i_output)\n",
    "model = Model(base_model.input, i_predictions)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.1396 - acc: 0.9518 - val_loss: 0.0602 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06018, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0915 - acc: 0.9671 - val_loss: 0.0448 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06018 to 0.04482, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0873 - acc: 0.9669 - val_loss: 0.0764 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04482\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0861 - acc: 0.9666 - val_loss: 0.0914 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04482\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0833 - acc: 0.9693 - val_loss: 0.0627 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04482\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0786 - acc: 0.9712 - val_loss: 0.0502 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04482\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0802 - acc: 0.9705 - val_loss: 0.0811 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04482\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 133s 7ms/step - loss: 0.0800 - acc: 0.9705 - val_loss: 0.0946 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc747985d68>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20,     # defalt 16\n",
    "\tepochs=8, \t# defalt 5\n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_4 0\n",
      "lambda_4 1\n",
      "conv2d_610 2\n",
      "batch_normalization_610 3\n",
      "activation_610 4\n",
      "conv2d_611 5\n",
      "batch_normalization_611 6\n",
      "activation_611 7\n",
      "conv2d_612 8\n",
      "batch_normalization_612 9\n",
      "activation_612 10\n",
      "max_pooling2d_13 11\n",
      "conv2d_613 12\n",
      "batch_normalization_613 13\n",
      "activation_613 14\n",
      "conv2d_614 15\n",
      "batch_normalization_614 16\n",
      "activation_614 17\n",
      "max_pooling2d_14 18\n",
      "conv2d_618 19\n",
      "batch_normalization_618 20\n",
      "activation_618 21\n",
      "conv2d_616 22\n",
      "conv2d_619 23\n",
      "batch_normalization_616 24\n",
      "batch_normalization_619 25\n",
      "activation_616 26\n",
      "activation_619 27\n",
      "average_pooling2d_4 28\n",
      "conv2d_615 29\n",
      "conv2d_617 30\n",
      "conv2d_620 31\n",
      "conv2d_621 32\n",
      "batch_normalization_615 33\n",
      "batch_normalization_617 34\n",
      "batch_normalization_620 35\n",
      "batch_normalization_621 36\n",
      "activation_615 37\n",
      "activation_617 38\n",
      "activation_620 39\n",
      "activation_621 40\n",
      "mixed_5b 41\n",
      "conv2d_625 42\n",
      "batch_normalization_625 43\n",
      "activation_625 44\n",
      "conv2d_623 45\n",
      "conv2d_626 46\n",
      "batch_normalization_623 47\n",
      "batch_normalization_626 48\n",
      "activation_623 49\n",
      "activation_626 50\n",
      "conv2d_622 51\n",
      "conv2d_624 52\n",
      "conv2d_627 53\n",
      "batch_normalization_622 54\n",
      "batch_normalization_624 55\n",
      "batch_normalization_627 56\n",
      "activation_622 57\n",
      "activation_624 58\n",
      "activation_627 59\n",
      "block35_1_mixed 60\n",
      "block35_1_conv 61\n",
      "block35_1 62\n",
      "block35_1_ac 63\n",
      "conv2d_631 64\n",
      "batch_normalization_631 65\n",
      "activation_631 66\n",
      "conv2d_629 67\n",
      "conv2d_632 68\n",
      "batch_normalization_629 69\n",
      "batch_normalization_632 70\n",
      "activation_629 71\n",
      "activation_632 72\n",
      "conv2d_628 73\n",
      "conv2d_630 74\n",
      "conv2d_633 75\n",
      "batch_normalization_628 76\n",
      "batch_normalization_630 77\n",
      "batch_normalization_633 78\n",
      "activation_628 79\n",
      "activation_630 80\n",
      "activation_633 81\n",
      "block35_2_mixed 82\n",
      "block35_2_conv 83\n",
      "block35_2 84\n",
      "block35_2_ac 85\n",
      "conv2d_637 86\n",
      "batch_normalization_637 87\n",
      "activation_637 88\n",
      "conv2d_635 89\n",
      "conv2d_638 90\n",
      "batch_normalization_635 91\n",
      "batch_normalization_638 92\n",
      "activation_635 93\n",
      "activation_638 94\n",
      "conv2d_634 95\n",
      "conv2d_636 96\n",
      "conv2d_639 97\n",
      "batch_normalization_634 98\n",
      "batch_normalization_636 99\n",
      "batch_normalization_639 100\n",
      "activation_634 101\n",
      "activation_636 102\n",
      "activation_639 103\n",
      "block35_3_mixed 104\n",
      "block35_3_conv 105\n",
      "block35_3 106\n",
      "block35_3_ac 107\n",
      "conv2d_643 108\n",
      "batch_normalization_643 109\n",
      "activation_643 110\n",
      "conv2d_641 111\n",
      "conv2d_644 112\n",
      "batch_normalization_641 113\n",
      "batch_normalization_644 114\n",
      "activation_641 115\n",
      "activation_644 116\n",
      "conv2d_640 117\n",
      "conv2d_642 118\n",
      "conv2d_645 119\n",
      "batch_normalization_640 120\n",
      "batch_normalization_642 121\n",
      "batch_normalization_645 122\n",
      "activation_640 123\n",
      "activation_642 124\n",
      "activation_645 125\n",
      "block35_4_mixed 126\n",
      "block35_4_conv 127\n",
      "block35_4 128\n",
      "block35_4_ac 129\n",
      "conv2d_649 130\n",
      "batch_normalization_649 131\n",
      "activation_649 132\n",
      "conv2d_647 133\n",
      "conv2d_650 134\n",
      "batch_normalization_647 135\n",
      "batch_normalization_650 136\n",
      "activation_647 137\n",
      "activation_650 138\n",
      "conv2d_646 139\n",
      "conv2d_648 140\n",
      "conv2d_651 141\n",
      "batch_normalization_646 142\n",
      "batch_normalization_648 143\n",
      "batch_normalization_651 144\n",
      "activation_646 145\n",
      "activation_648 146\n",
      "activation_651 147\n",
      "block35_5_mixed 148\n",
      "block35_5_conv 149\n",
      "block35_5 150\n",
      "block35_5_ac 151\n",
      "conv2d_655 152\n",
      "batch_normalization_655 153\n",
      "activation_655 154\n",
      "conv2d_653 155\n",
      "conv2d_656 156\n",
      "batch_normalization_653 157\n",
      "batch_normalization_656 158\n",
      "activation_653 159\n",
      "activation_656 160\n",
      "conv2d_652 161\n",
      "conv2d_654 162\n",
      "conv2d_657 163\n",
      "batch_normalization_652 164\n",
      "batch_normalization_654 165\n",
      "batch_normalization_657 166\n",
      "activation_652 167\n",
      "activation_654 168\n",
      "activation_657 169\n",
      "block35_6_mixed 170\n",
      "block35_6_conv 171\n",
      "block35_6 172\n",
      "block35_6_ac 173\n",
      "conv2d_661 174\n",
      "batch_normalization_661 175\n",
      "activation_661 176\n",
      "conv2d_659 177\n",
      "conv2d_662 178\n",
      "batch_normalization_659 179\n",
      "batch_normalization_662 180\n",
      "activation_659 181\n",
      "activation_662 182\n",
      "conv2d_658 183\n",
      "conv2d_660 184\n",
      "conv2d_663 185\n",
      "batch_normalization_658 186\n",
      "batch_normalization_660 187\n",
      "batch_normalization_663 188\n",
      "activation_658 189\n",
      "activation_660 190\n",
      "activation_663 191\n",
      "block35_7_mixed 192\n",
      "block35_7_conv 193\n",
      "block35_7 194\n",
      "block35_7_ac 195\n",
      "conv2d_667 196\n",
      "batch_normalization_667 197\n",
      "activation_667 198\n",
      "conv2d_665 199\n",
      "conv2d_668 200\n",
      "batch_normalization_665 201\n",
      "batch_normalization_668 202\n",
      "activation_665 203\n",
      "activation_668 204\n",
      "conv2d_664 205\n",
      "conv2d_666 206\n",
      "conv2d_669 207\n",
      "batch_normalization_664 208\n",
      "batch_normalization_666 209\n",
      "batch_normalization_669 210\n",
      "activation_664 211\n",
      "activation_666 212\n",
      "activation_669 213\n",
      "block35_8_mixed 214\n",
      "block35_8_conv 215\n",
      "block35_8 216\n",
      "block35_8_ac 217\n",
      "conv2d_673 218\n",
      "batch_normalization_673 219\n",
      "activation_673 220\n",
      "conv2d_671 221\n",
      "conv2d_674 222\n",
      "batch_normalization_671 223\n",
      "batch_normalization_674 224\n",
      "activation_671 225\n",
      "activation_674 226\n",
      "conv2d_670 227\n",
      "conv2d_672 228\n",
      "conv2d_675 229\n",
      "batch_normalization_670 230\n",
      "batch_normalization_672 231\n",
      "batch_normalization_675 232\n",
      "activation_670 233\n",
      "activation_672 234\n",
      "activation_675 235\n",
      "block35_9_mixed 236\n",
      "block35_9_conv 237\n",
      "block35_9 238\n",
      "block35_9_ac 239\n",
      "conv2d_679 240\n",
      "batch_normalization_679 241\n",
      "activation_679 242\n",
      "conv2d_677 243\n",
      "conv2d_680 244\n",
      "batch_normalization_677 245\n",
      "batch_normalization_680 246\n",
      "activation_677 247\n",
      "activation_680 248\n",
      "conv2d_676 249\n",
      "conv2d_678 250\n",
      "conv2d_681 251\n",
      "batch_normalization_676 252\n",
      "batch_normalization_678 253\n",
      "batch_normalization_681 254\n",
      "activation_676 255\n",
      "activation_678 256\n",
      "activation_681 257\n",
      "block35_10_mixed 258\n",
      "block35_10_conv 259\n",
      "block35_10 260\n",
      "block35_10_ac 261\n",
      "conv2d_683 262\n",
      "batch_normalization_683 263\n",
      "activation_683 264\n",
      "conv2d_684 265\n",
      "batch_normalization_684 266\n",
      "activation_684 267\n",
      "conv2d_682 268\n",
      "conv2d_685 269\n",
      "batch_normalization_682 270\n",
      "batch_normalization_685 271\n",
      "activation_682 272\n",
      "activation_685 273\n",
      "max_pooling2d_15 274\n",
      "mixed_6a 275\n",
      "conv2d_687 276\n",
      "batch_normalization_687 277\n",
      "activation_687 278\n",
      "conv2d_688 279\n",
      "batch_normalization_688 280\n",
      "activation_688 281\n",
      "conv2d_686 282\n",
      "conv2d_689 283\n",
      "batch_normalization_686 284\n",
      "batch_normalization_689 285\n",
      "activation_686 286\n",
      "activation_689 287\n",
      "block17_1_mixed 288\n",
      "block17_1_conv 289\n",
      "block17_1 290\n",
      "block17_1_ac 291\n",
      "conv2d_691 292\n",
      "batch_normalization_691 293\n",
      "activation_691 294\n",
      "conv2d_692 295\n",
      "batch_normalization_692 296\n",
      "activation_692 297\n",
      "conv2d_690 298\n",
      "conv2d_693 299\n",
      "batch_normalization_690 300\n",
      "batch_normalization_693 301\n",
      "activation_690 302\n",
      "activation_693 303\n",
      "block17_2_mixed 304\n",
      "block17_2_conv 305\n",
      "block17_2 306\n",
      "block17_2_ac 307\n",
      "conv2d_695 308\n",
      "batch_normalization_695 309\n",
      "activation_695 310\n",
      "conv2d_696 311\n",
      "batch_normalization_696 312\n",
      "activation_696 313\n",
      "conv2d_694 314\n",
      "conv2d_697 315\n",
      "batch_normalization_694 316\n",
      "batch_normalization_697 317\n",
      "activation_694 318\n",
      "activation_697 319\n",
      "block17_3_mixed 320\n",
      "block17_3_conv 321\n",
      "block17_3 322\n",
      "block17_3_ac 323\n",
      "conv2d_699 324\n",
      "batch_normalization_699 325\n",
      "activation_699 326\n",
      "conv2d_700 327\n",
      "batch_normalization_700 328\n",
      "activation_700 329\n",
      "conv2d_698 330\n",
      "conv2d_701 331\n",
      "batch_normalization_698 332\n",
      "batch_normalization_701 333\n",
      "activation_698 334\n",
      "activation_701 335\n",
      "block17_4_mixed 336\n",
      "block17_4_conv 337\n",
      "block17_4 338\n",
      "block17_4_ac 339\n",
      "conv2d_703 340\n",
      "batch_normalization_703 341\n",
      "activation_703 342\n",
      "conv2d_704 343\n",
      "batch_normalization_704 344\n",
      "activation_704 345\n",
      "conv2d_702 346\n",
      "conv2d_705 347\n",
      "batch_normalization_702 348\n",
      "batch_normalization_705 349\n",
      "activation_702 350\n",
      "activation_705 351\n",
      "block17_5_mixed 352\n",
      "block17_5_conv 353\n",
      "block17_5 354\n",
      "block17_5_ac 355\n",
      "conv2d_707 356\n",
      "batch_normalization_707 357\n",
      "activation_707 358\n",
      "conv2d_708 359\n",
      "batch_normalization_708 360\n",
      "activation_708 361\n",
      "conv2d_706 362\n",
      "conv2d_709 363\n",
      "batch_normalization_706 364\n",
      "batch_normalization_709 365\n",
      "activation_706 366\n",
      "activation_709 367\n",
      "block17_6_mixed 368\n",
      "block17_6_conv 369\n",
      "block17_6 370\n",
      "block17_6_ac 371\n",
      "conv2d_711 372\n",
      "batch_normalization_711 373\n",
      "activation_711 374\n",
      "conv2d_712 375\n",
      "batch_normalization_712 376\n",
      "activation_712 377\n",
      "conv2d_710 378\n",
      "conv2d_713 379\n",
      "batch_normalization_710 380\n",
      "batch_normalization_713 381\n",
      "activation_710 382\n",
      "activation_713 383\n",
      "block17_7_mixed 384\n",
      "block17_7_conv 385\n",
      "block17_7 386\n",
      "block17_7_ac 387\n",
      "conv2d_715 388\n",
      "batch_normalization_715 389\n",
      "activation_715 390\n",
      "conv2d_716 391\n",
      "batch_normalization_716 392\n",
      "activation_716 393\n",
      "conv2d_714 394\n",
      "conv2d_717 395\n",
      "batch_normalization_714 396\n",
      "batch_normalization_717 397\n",
      "activation_714 398\n",
      "activation_717 399\n",
      "block17_8_mixed 400\n",
      "block17_8_conv 401\n",
      "block17_8 402\n",
      "block17_8_ac 403\n",
      "conv2d_719 404\n",
      "batch_normalization_719 405\n",
      "activation_719 406\n",
      "conv2d_720 407\n",
      "batch_normalization_720 408\n",
      "activation_720 409\n",
      "conv2d_718 410\n",
      "conv2d_721 411\n",
      "batch_normalization_718 412\n",
      "batch_normalization_721 413\n",
      "activation_718 414\n",
      "activation_721 415\n",
      "block17_9_mixed 416\n",
      "block17_9_conv 417\n",
      "block17_9 418\n",
      "block17_9_ac 419\n",
      "conv2d_723 420\n",
      "batch_normalization_723 421\n",
      "activation_723 422\n",
      "conv2d_724 423\n",
      "batch_normalization_724 424\n",
      "activation_724 425\n",
      "conv2d_722 426\n",
      "conv2d_725 427\n",
      "batch_normalization_722 428\n",
      "batch_normalization_725 429\n",
      "activation_722 430\n",
      "activation_725 431\n",
      "block17_10_mixed 432\n",
      "block17_10_conv 433\n",
      "block17_10 434\n",
      "block17_10_ac 435\n",
      "conv2d_727 436\n",
      "batch_normalization_727 437\n",
      "activation_727 438\n",
      "conv2d_728 439\n",
      "batch_normalization_728 440\n",
      "activation_728 441\n",
      "conv2d_726 442\n",
      "conv2d_729 443\n",
      "batch_normalization_726 444\n",
      "batch_normalization_729 445\n",
      "activation_726 446\n",
      "activation_729 447\n",
      "block17_11_mixed 448\n",
      "block17_11_conv 449\n",
      "block17_11 450\n",
      "block17_11_ac 451\n",
      "conv2d_731 452\n",
      "batch_normalization_731 453\n",
      "activation_731 454\n",
      "conv2d_732 455\n",
      "batch_normalization_732 456\n",
      "activation_732 457\n",
      "conv2d_730 458\n",
      "conv2d_733 459\n",
      "batch_normalization_730 460\n",
      "batch_normalization_733 461\n",
      "activation_730 462\n",
      "activation_733 463\n",
      "block17_12_mixed 464\n",
      "block17_12_conv 465\n",
      "block17_12 466\n",
      "block17_12_ac 467\n",
      "conv2d_735 468\n",
      "batch_normalization_735 469\n",
      "activation_735 470\n",
      "conv2d_736 471\n",
      "batch_normalization_736 472\n",
      "activation_736 473\n",
      "conv2d_734 474\n",
      "conv2d_737 475\n",
      "batch_normalization_734 476\n",
      "batch_normalization_737 477\n",
      "activation_734 478\n",
      "activation_737 479\n",
      "block17_13_mixed 480\n",
      "block17_13_conv 481\n",
      "block17_13 482\n",
      "block17_13_ac 483\n",
      "conv2d_739 484\n",
      "batch_normalization_739 485\n",
      "activation_739 486\n",
      "conv2d_740 487\n",
      "batch_normalization_740 488\n",
      "activation_740 489\n",
      "conv2d_738 490\n",
      "conv2d_741 491\n",
      "batch_normalization_738 492\n",
      "batch_normalization_741 493\n",
      "activation_738 494\n",
      "activation_741 495\n",
      "block17_14_mixed 496\n",
      "block17_14_conv 497\n",
      "block17_14 498\n",
      "block17_14_ac 499\n",
      "conv2d_743 500\n",
      "batch_normalization_743 501\n",
      "activation_743 502\n",
      "conv2d_744 503\n",
      "batch_normalization_744 504\n",
      "activation_744 505\n",
      "conv2d_742 506\n",
      "conv2d_745 507\n",
      "batch_normalization_742 508\n",
      "batch_normalization_745 509\n",
      "activation_742 510\n",
      "activation_745 511\n",
      "block17_15_mixed 512\n",
      "block17_15_conv 513\n",
      "block17_15 514\n",
      "block17_15_ac 515\n",
      "conv2d_747 516\n",
      "batch_normalization_747 517\n",
      "activation_747 518\n",
      "conv2d_748 519\n",
      "batch_normalization_748 520\n",
      "activation_748 521\n",
      "conv2d_746 522\n",
      "conv2d_749 523\n",
      "batch_normalization_746 524\n",
      "batch_normalization_749 525\n",
      "activation_746 526\n",
      "activation_749 527\n",
      "block17_16_mixed 528\n",
      "block17_16_conv 529\n",
      "block17_16 530\n",
      "block17_16_ac 531\n",
      "conv2d_751 532\n",
      "batch_normalization_751 533\n",
      "activation_751 534\n",
      "conv2d_752 535\n",
      "batch_normalization_752 536\n",
      "activation_752 537\n",
      "conv2d_750 538\n",
      "conv2d_753 539\n",
      "batch_normalization_750 540\n",
      "batch_normalization_753 541\n",
      "activation_750 542\n",
      "activation_753 543\n",
      "block17_17_mixed 544\n",
      "block17_17_conv 545\n",
      "block17_17 546\n",
      "block17_17_ac 547\n",
      "conv2d_755 548\n",
      "batch_normalization_755 549\n",
      "activation_755 550\n",
      "conv2d_756 551\n",
      "batch_normalization_756 552\n",
      "activation_756 553\n",
      "conv2d_754 554\n",
      "conv2d_757 555\n",
      "batch_normalization_754 556\n",
      "batch_normalization_757 557\n",
      "activation_754 558\n",
      "activation_757 559\n",
      "block17_18_mixed 560\n",
      "block17_18_conv 561\n",
      "block17_18 562\n",
      "block17_18_ac 563\n",
      "conv2d_759 564\n",
      "batch_normalization_759 565\n",
      "activation_759 566\n",
      "conv2d_760 567\n",
      "batch_normalization_760 568\n",
      "activation_760 569\n",
      "conv2d_758 570\n",
      "conv2d_761 571\n",
      "batch_normalization_758 572\n",
      "batch_normalization_761 573\n",
      "activation_758 574\n",
      "activation_761 575\n",
      "block17_19_mixed 576\n",
      "block17_19_conv 577\n",
      "block17_19 578\n",
      "block17_19_ac 579\n",
      "conv2d_763 580\n",
      "batch_normalization_763 581\n",
      "activation_763 582\n",
      "conv2d_764 583\n",
      "batch_normalization_764 584\n",
      "activation_764 585\n",
      "conv2d_762 586\n",
      "conv2d_765 587\n",
      "batch_normalization_762 588\n",
      "batch_normalization_765 589\n",
      "activation_762 590\n",
      "activation_765 591\n",
      "block17_20_mixed 592\n",
      "block17_20_conv 593\n",
      "block17_20 594\n",
      "block17_20_ac 595\n",
      "conv2d_770 596\n",
      "batch_normalization_770 597\n",
      "activation_770 598\n",
      "conv2d_766 599\n",
      "conv2d_768 600\n",
      "conv2d_771 601\n",
      "batch_normalization_766 602\n",
      "batch_normalization_768 603\n",
      "batch_normalization_771 604\n",
      "activation_766 605\n",
      "activation_768 606\n",
      "activation_771 607\n",
      "conv2d_767 608\n",
      "conv2d_769 609\n",
      "conv2d_772 610\n",
      "batch_normalization_767 611\n",
      "batch_normalization_769 612\n",
      "batch_normalization_772 613\n",
      "activation_767 614\n",
      "activation_769 615\n",
      "activation_772 616\n",
      "max_pooling2d_16 617\n",
      "mixed_7a 618\n",
      "conv2d_774 619\n",
      "batch_normalization_774 620\n",
      "activation_774 621\n",
      "conv2d_775 622\n",
      "batch_normalization_775 623\n",
      "activation_775 624\n",
      "conv2d_773 625\n",
      "conv2d_776 626\n",
      "batch_normalization_773 627\n",
      "batch_normalization_776 628\n",
      "activation_773 629\n",
      "activation_776 630\n",
      "block8_1_mixed 631\n",
      "block8_1_conv 632\n",
      "block8_1 633\n",
      "block8_1_ac 634\n",
      "conv2d_778 635\n",
      "batch_normalization_778 636\n",
      "activation_778 637\n",
      "conv2d_779 638\n",
      "batch_normalization_779 639\n",
      "activation_779 640\n",
      "conv2d_777 641\n",
      "conv2d_780 642\n",
      "batch_normalization_777 643\n",
      "batch_normalization_780 644\n",
      "activation_777 645\n",
      "activation_780 646\n",
      "block8_2_mixed 647\n",
      "block8_2_conv 648\n",
      "block8_2 649\n",
      "block8_2_ac 650\n",
      "conv2d_782 651\n",
      "batch_normalization_782 652\n",
      "activation_782 653\n",
      "conv2d_783 654\n",
      "batch_normalization_783 655\n",
      "activation_783 656\n",
      "conv2d_781 657\n",
      "conv2d_784 658\n",
      "batch_normalization_781 659\n",
      "batch_normalization_784 660\n",
      "activation_781 661\n",
      "activation_784 662\n",
      "block8_3_mixed 663\n",
      "block8_3_conv 664\n",
      "block8_3 665\n",
      "block8_3_ac 666\n",
      "conv2d_786 667\n",
      "batch_normalization_786 668\n",
      "activation_786 669\n",
      "conv2d_787 670\n",
      "batch_normalization_787 671\n",
      "activation_787 672\n",
      "conv2d_785 673\n",
      "conv2d_788 674\n",
      "batch_normalization_785 675\n",
      "batch_normalization_788 676\n",
      "activation_785 677\n",
      "activation_788 678\n",
      "block8_4_mixed 679\n",
      "block8_4_conv 680\n",
      "block8_4 681\n",
      "block8_4_ac 682\n",
      "conv2d_790 683\n",
      "batch_normalization_790 684\n",
      "activation_790 685\n",
      "conv2d_791 686\n",
      "batch_normalization_791 687\n",
      "activation_791 688\n",
      "conv2d_789 689\n",
      "conv2d_792 690\n",
      "batch_normalization_789 691\n",
      "batch_normalization_792 692\n",
      "activation_789 693\n",
      "activation_792 694\n",
      "block8_5_mixed 695\n",
      "block8_5_conv 696\n",
      "block8_5 697\n",
      "block8_5_ac 698\n",
      "conv2d_794 699\n",
      "batch_normalization_794 700\n",
      "activation_794 701\n",
      "conv2d_795 702\n",
      "batch_normalization_795 703\n",
      "activation_795 704\n",
      "conv2d_793 705\n",
      "conv2d_796 706\n",
      "batch_normalization_793 707\n",
      "batch_normalization_796 708\n",
      "activation_793 709\n",
      "activation_796 710\n",
      "block8_6_mixed 711\n",
      "block8_6_conv 712\n",
      "block8_6 713\n",
      "block8_6_ac 714\n",
      "conv2d_798 715\n",
      "batch_normalization_798 716\n",
      "activation_798 717\n",
      "conv2d_799 718\n",
      "batch_normalization_799 719\n",
      "activation_799 720\n",
      "conv2d_797 721\n",
      "conv2d_800 722\n",
      "batch_normalization_797 723\n",
      "batch_normalization_800 724\n",
      "activation_797 725\n",
      "activation_800 726\n",
      "block8_7_mixed 727\n",
      "block8_7_conv 728\n",
      "block8_7 729\n",
      "block8_7_ac 730\n",
      "conv2d_802 731\n",
      "batch_normalization_802 732\n",
      "activation_802 733\n",
      "conv2d_803 734\n",
      "batch_normalization_803 735\n",
      "activation_803 736\n",
      "conv2d_801 737\n",
      "conv2d_804 738\n",
      "batch_normalization_801 739\n",
      "batch_normalization_804 740\n",
      "activation_801 741\n",
      "activation_804 742\n",
      "block8_8_mixed 743\n",
      "block8_8_conv 744\n",
      "block8_8 745\n",
      "block8_8_ac 746\n",
      "conv2d_806 747\n",
      "batch_normalization_806 748\n",
      "activation_806 749\n",
      "conv2d_807 750\n",
      "batch_normalization_807 751\n",
      "activation_807 752\n",
      "conv2d_805 753\n",
      "conv2d_808 754\n",
      "batch_normalization_805 755\n",
      "batch_normalization_808 756\n",
      "activation_805 757\n",
      "activation_808 758\n",
      "block8_9_mixed 759\n",
      "block8_9_conv 760\n",
      "block8_9 761\n",
      "block8_9_ac 762\n",
      "conv2d_810 763\n",
      "batch_normalization_810 764\n",
      "activation_810 765\n",
      "conv2d_811 766\n",
      "batch_normalization_811 767\n",
      "activation_811 768\n",
      "conv2d_809 769\n",
      "conv2d_812 770\n",
      "batch_normalization_809 771\n",
      "batch_normalization_812 772\n",
      "activation_809 773\n",
      "activation_812 774\n",
      "block8_10_mixed 775\n",
      "block8_10_conv 776\n",
      "block8_10 777\n",
      "conv_7b 778\n",
      "conv_7b_bn 779\n",
      "conv_7b_ac 780\n",
      "global_average_pooling2d_4 781\n",
      "dropout_4 782\n",
      "dense_4 783\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    print(model.layers[i].name, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放开靠后部分网络层进行 fine tune 微调模型\n",
    "for layer in model.layers[770:]:\t# total 783 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 166s 8ms/step - loss: 0.0693 - acc: 0.9753 - val_loss: 0.0416 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.04482 to 0.04160, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0726 - acc: 0.9722 - val_loss: 0.0414 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04160 to 0.04142, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0732 - acc: 0.9724 - val_loss: 0.0398 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04142 to 0.03980, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0702 - acc: 0.9723 - val_loss: 0.0389 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03980 to 0.03886, saving model to weights_checkpint_030_770.hdf5\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0659 - acc: 0.9743 - val_loss: 0.0409 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03886\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0676 - acc: 0.9747 - val_loss: 0.0403 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03886\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0699 - acc: 0.9737 - val_loss: 0.0397 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03886\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.0635 - acc: 0.9773 - val_loss: 0.0386 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03886 to 0.03862, saving model to weights_checkpint_030_770.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc745e02048>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, \t# first: 16\n",
    "\tepochs=8, \t# first: 5 \n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "         verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelInceptionResNet_transfer_Doup030_770layer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取测试图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:28<00:00, 432.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((n, img_height, img_width, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(12500)):\n",
    "    j = i+1\n",
    "    X_test[i] = cv2.resize(cv2.imread('test/%d.jpg' % j), (img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 154s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for i in range(12500):\n",
    "\tdf.set_value(i, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred_changePredictCsvMethond_Drop030_770layer.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
